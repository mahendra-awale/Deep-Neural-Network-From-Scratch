{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network For Image Classification From Scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to load the training and test set\n",
    "import os\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#=====================================\n",
    "def load_data(training_set, test_set):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    training_set -- folder containing training set images (image name == label)\n",
    "    test_set -- folder containng test set images (image name == label)\n",
    "    \n",
    "    Returns:\n",
    "    training_set_features -- 2d array of size (nx, m)\n",
    "    training_set_labels -- 2d array of size (1, m)\n",
    "    test_set_features -- 2d array of size (nx, m)\n",
    "    test_set_labels -- 2d array of size (1, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the training set data\n",
    "    infiles = os.listdir(training_set)\n",
    "    training_set_features = []\n",
    "    training_set_labels = []\n",
    "    training_set_features = []\n",
    "    training_set_labels = []\n",
    "    \n",
    "    for image in infiles:\n",
    "        label = image.split(\".\")[0]\n",
    "        if label == \"cat\":\n",
    "            training_set_labels.append(0)\n",
    "        else:\n",
    "            training_set_labels.append(1)\n",
    "        im = plt.imread(training_set+ \"/\" + image)\n",
    "        shape = im.shape\n",
    "        features = im.reshape(shape[0] * shape[1] * shape[2])\n",
    "        training_set_features.append(features)\n",
    "    \n",
    "    training_set_features = np.asarray(training_set_features, dtype=float)\n",
    "    training_set_features = training_set_features.T\n",
    "    training_set_labels = np.asarray(training_set_labels, dtype=float)\n",
    "    training_set_labels = training_set_labels.reshape(1, len(training_set_labels))\n",
    "    \n",
    "    #Load the test set data\n",
    "    infiles = os.listdir(test_set)\n",
    "    test_set_features = []\n",
    "    test_set_labels = []\n",
    "    test_set_features = []\n",
    "    test_set_labels = []\n",
    "    \n",
    "    for image in infiles:\n",
    "        label = image.split(\".\")[0]\n",
    "        if label == \"cat\":\n",
    "            test_set_labels.append(0)\n",
    "        else:\n",
    "            test_set_labels.append(1)\n",
    "        im = plt.imread(test_set+ \"/\" + image)\n",
    "        shape = im.shape\n",
    "        features = im.reshape(shape[0] * shape[1] * shape[2])\n",
    "        test_set_features.append(features)\n",
    "    \n",
    "    test_set_features = np.asarray(test_set_features, dtype=float)\n",
    "    test_set_features = test_set_features.T\n",
    "    test_set_labels = np.asarray(test_set_labels, dtype=float)\n",
    "    test_set_labels = test_set_labels.reshape(1, len(test_set_labels))\n",
    "    \n",
    "    return training_set_features, training_set_labels, test_set_features, test_set_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the data\n",
    "train_f, train_l, test_f, test_l = load_data(\"train_set2\", \"test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h1, n_h2, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h1 -- size of the 1st hidden layer\n",
    "    n_h2 -- size of the 2nd hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h1, n_x)\n",
    "                    b1 -- bias vector of shape (n_h1, 1)\n",
    "                    W2 -- weight matrix of shape (n_h2, n_h1)\n",
    "                    b2 -- bias vector of shape (n_h2, 1)\n",
    "                    W3 -- weight matrix of shape (n_y, n_h2)\n",
    "                    b3 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h1, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h1, 1))\n",
    "    W2 = np.random.randn(n_h2, n_h1) * 0.01\n",
    "    b2 = np.zeros((n_h2, 1))\n",
    "    W3 = np.random.randn(n_y, n_h2) * 0.01\n",
    "    b3 = np.zeros((n_y, 1))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h1, n_x))\n",
    "    assert (b1.shape == (n_h1, 1))\n",
    "    assert (W2.shape == (n_h2, n_h1))\n",
    "    assert (b2.shape == (n_h2, 1))\n",
    "    assert (W3.shape == (n_y, n_h2))\n",
    "    assert (b3.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3\n",
    "                 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(30000, 10, 5, 1)\n",
    "print(parameters[\"W1\"].shape, \"W1\")\n",
    "print(parameters[\"b1\"].shape, \"b1\")\n",
    "print(parameters[\"W2\"].shape, \"W2\")\n",
    "print(parameters[\"b2\"].shape, \"b2\")\n",
    "print(parameters[\"W3\"].shape, \"W3\")\n",
    "print(parameters[\"b3\"].shape, \"b3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z, takederivative):\n",
    "    \n",
    "    s = 1/ (1 + np.exp(-z))\n",
    "    if takederivative == \"derivative\":\n",
    "        return s * (1 - s)\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propogation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\", \"A2\", and \"Z3\" , \"A3\n",
    "    \"\"\"\n",
    "    Z1 = np.dot(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
    "    A1 = sigmoid(Z1, \"notderivative\")\n",
    "    \n",
    "    Z2 = np.dot(parameters[\"W2\"], A1) + parameters[\"b2\"]\n",
    "    A2 = sigmoid(Z2, \"notderivative\")\n",
    "    \n",
    "    Z3 = np.dot(parameters[\"W3\"], A2) + parameters[\"b3\"]\n",
    "    A3 = sigmoid(Z3, \"notderivative\")\n",
    "    \n",
    "    cache = {\"Z1\" : Z1,\n",
    "             \"A1\" : A1,\n",
    "             \"Z2\" : Z2,\n",
    "             \"A2\" : A2,\n",
    "             \"Z3\" : Z3,\n",
    "             \"A3\" : A3}\n",
    "    \n",
    "    return A3, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(30000, 10, 5, 1)\n",
    "A3, cache = forward_propogation(train_f, parameters)\n",
    "print(A3.shape, \"A2\")\n",
    "print(cache[\"Z1\"].shape, \"Z1\")\n",
    "print(cache[\"A1\"].shape, \"A1\")\n",
    "print(cache[\"Z2\"].shape, \"Z2\")\n",
    "print(cache[\"A2\"].shape, \"A2\")\n",
    "print(cache[\"Z3\"].shape, \"Z3\")\n",
    "print(cache[\"A3\"].shape, \"A3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    m = A3.shape[1]\n",
    "    cost = -1 / m * (np.dot(Y, np.log(A3).T) + np.dot(1 - Y, np.log(1 - A3).T));\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(A3, train_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_propogation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\", \"A2\" and Z3 and A3.\n",
    "    X -- input data of shape (nx, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A3 = cache[\"A3\"]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1/m * (np.dot(dZ3, A2.T))\n",
    "    db3 =  1/m * (np.sum(dZ3, axis=1, keepdims=True))\n",
    "    \n",
    "    dZ2 = np.dot(W3.T, dZ3) * sigmoid(cache[\"Z2\"], \"derivative\")\n",
    "    dW2 = 1/m *  (np.dot(dZ2, A1.T))\n",
    "    db2 =  1/m * (np.sum(dZ2, axis=1, keepdims=True))\n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * sigmoid(cache[\"Z1\"], \"derivative\")\n",
    "    dW1 = 1/m *  (np.dot(dZ1, X.T))\n",
    "    db1 =  1/m * (np.sum(dZ1, axis=1, keepdims=True))\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = back_propogation(parameters, cache, train_f, train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dW3 = grads[\"dW3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = W1 - (learning_rate * dW1)\n",
    "    b1 = b1 - (learning_rate * db1)\n",
    "    W2 = W2 - (learning_rate * dW2)\n",
    "    b2 = b2 - (learning_rate * db2)\n",
    "    W3 = W3 - (learning_rate * dW3)\n",
    "    b3 = b3 - (learning_rate * db3)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_parameters(parameters, grads, learning_rate = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h1, n_h2, num_iterations = 10000, print_cost=False, learning_rate = 0.0075):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    parameters = initialize_parameters(n_x, n_h1, n_h2, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A3, cache = forward_propogation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A3, Y)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = back_propogation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # train set labels\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "\n",
    "X, Y = load_planar_dataset()\n",
    "parameters = nn_model(train_set_x, train_set_y, 50, 10, num_iterations=10000, print_cost=True, learning_rate=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = [1 if x >= 0.5 else 0 for x in A2[0]]\n",
    "    predictions = np.asarray(predictions)\n",
    "    predictions = predictions.reshape(A2.shape)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
